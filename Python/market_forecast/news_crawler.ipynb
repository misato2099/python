{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 新聞爬蟲器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import time, os, re\n",
    "import feedparser\n",
    "import schedule"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 爬蟲檔名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_tw = 'news_tw.csv'\n",
    "csv_file_us = 'news_us.csv'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 台灣新聞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yahoo財經台灣\n",
    "def crawler_tw():\n",
    "    #clear_output(wait=True)\n",
    "    global csv_file_tw\n",
    "    url1 = 'https://tw.finance.yahoo.com/tw-market'\n",
    "    url2 = 'https://tw.finance.yahoo.com/news'\n",
    "\n",
    "    # 載入網頁\n",
    "    option = webdriver.ChromeOptions()\n",
    "    option.add_argument('headless')\n",
    "    option.add_argument('--no-sandbox')\n",
    "    option.add_argument('--disable-dev-shm-usage')\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service)\n",
    "    driver.get(url1)\n",
    "\n",
    "    # 模擬滾動到最底\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)  # 等待網頁更新\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    # 取得網頁內容\n",
    "    html = driver.page_source\n",
    "\n",
    "    # 解析 HTML 內容\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # 找到包含title的元素，並獲取標題及source\n",
    "    titles1 = soup.find_all('a', class_='Fw(b) Fz(20px) Fz(16px)--mobile Lh(23px) Lh(1.38)--mobile C($c-primary-text)! C($c-active-text)!:h LineClamp(2,46px)!--mobile LineClamp(2,46px)!--sm1024 mega-item-header-link Td(n) C(#0078ff):h C(#000) LineClamp(2,46px) LineClamp(2,38px)--sm1024 not-isInStreamVideoEnabled')\n",
    "    sources1 = soup.find_all('div', class_='C(#959595) Fz(13px) C($c-secondary-text)! D(ib) Mb(6px)')\n",
    "\n",
    "    # 關閉瀏覽器\n",
    "    driver.quit()\n",
    "\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service) \n",
    "    driver.get(url2)\n",
    "\n",
    "    # 模擬滾動到最底\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)  # 等待網頁更新\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    # 取得網頁內容\n",
    "    html = driver.page_source\n",
    "\n",
    "    # 解析 HTML 內容\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # 找到包含title的元素，並獲取標題及source\n",
    "    titles2 = soup.find_all('a', class_='Fw(b) Fz(20px) Fz(16px)--mobile Lh(23px) Lh(1.38)--mobile C($c-primary-text)! C($c-active-text)!:h LineClamp(2,46px)!--mobile LineClamp(2,46px)!--sm1024 mega-item-header-link Td(n) C(#0078ff):h C(#000) LineClamp(2,46px) LineClamp(2,38px)--sm1024 not-isInStreamVideoEnabled')\n",
    "    sources2 = soup.find_all('div', class_='C(#959595) Fz(13px) C($c-secondary-text)! D(ib) Mb(6px)')\n",
    "\n",
    "    # 關閉瀏覽器\n",
    "    driver.quit()\n",
    "\n",
    "    # 建立資料框架\n",
    "    #csv_file = 'title_raw.csv'\n",
    "    \n",
    "    # 檢查檔案是否存在\n",
    "    if not os.path.exists(csv_file_tw):\n",
    "        # 若檔案不存在，建立空的資料框並儲存為 CSV 檔案\n",
    "        empty_df = pd.DataFrame(columns=['crawl_date', 'source_date', 'title', 'source', 'source_time', 'Sentiment_label'])\n",
    "        empty_df.to_csv(csv_file_tw, index=False, encoding='utf-8-sig')\n",
    "    df_tw = pd.read_csv(csv_file_tw, encoding='utf-8-sig')\n",
    "\n",
    "    # 逐個獲取並添加爬取的資料至資料框架\n",
    "    for title, source in zip(titles1, sources1):\n",
    "        title_text = title.text.strip()\n",
    "        source_text = source.text.strip()\n",
    "        date = datetime.now().strftime('%Y/%m/%d %H:%M')\n",
    "\n",
    "        # 分割「source」中的資料到「source_time」\n",
    "        source_parts = source_text.split('•')\n",
    "        if len(source_parts) > 1:\n",
    "            source_text = source_parts[0].strip()\n",
    "            source_time = source_parts[1].strip()\n",
    "        else:\n",
    "            source_time = None\n",
    "\n",
    "        # 添加資料至資料框架\n",
    "        new_data = pd.DataFrame({'crawl_date': [date], 'title': [title_text], 'source': [source_text], 'source_time': [source_time]})\n",
    "        df_tw = pd.concat([df_tw, new_data], ignore_index=True)\n",
    " \n",
    "    for title, source in zip(titles2, sources2):\n",
    "        title_text = title.text.strip()\n",
    "        source_text = source.text.strip()\n",
    "        date = datetime.now().strftime('%Y/%m/%d %H:%M')\n",
    "\n",
    "        # 分割「source」中的資料到「source_date」\n",
    "        source_parts = source_text.split('•')\n",
    "        if len(source_parts) > 1:\n",
    "            source_text = source_parts[0].strip()\n",
    "            source_time = source_parts[1].strip()\n",
    "        else:\n",
    "            source_time = None\n",
    "\n",
    "        # 添加資料至資料框架\n",
    "        new_data = pd.DataFrame({'crawl_date': [date], 'title': [title_text], 'source': [source_text], 'source_time': [source_time]})\n",
    "        df_tw = pd.concat([df_tw, new_data], ignore_index=True)\n",
    "         \n",
    "    # 將資料框架輸出為 CSV 檔案\n",
    "    df_tw.to_csv(csv_file_tw, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    print(f'爬蟲資料已儲存至{csv_file_tw}檔案中。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def source_date_tw(csvfile):\n",
    "    clear_output(wait=True)\n",
    "    global csv_file_tw\n",
    "    df_tw = pd.read_csv(csvfile)\n",
    "    # df_tw = pd.read_csv(csvfile, usecols=['crawl_date', 'source_date', 'title', 'source', 'source_time'])\n",
    "    # 逐行處理資料\n",
    "    for index, row in df_tw.iterrows():\n",
    "        crawl_date = datetime.strptime(row['crawl_date'], '%Y/%m/%d %H:%M')\n",
    "        source_time = row['source_time']\n",
    "\n",
    "        # 判斷source_time的類型\n",
    "        if '秒' in source_time:\n",
    "            sec = int(source_time.split(' ')[0])\n",
    "            source_date = crawl_date - timedelta(seconds=sec)\n",
    "        elif '小時' in source_time:\n",
    "            hr = int(source_time.split(' ')[0])\n",
    "            source_date = crawl_date - timedelta(hours=hr)\n",
    "        elif '分鐘' in source_time:\n",
    "            min = int(source_time.split(' ')[0])\n",
    "            source_date = crawl_date - timedelta(minutes=min)\n",
    "        elif source_time == '昨天':\n",
    "            source_date = crawl_date - timedelta(days=1)\n",
    "        elif source_time == '前天':\n",
    "            source_date = crawl_date - timedelta(days=2)\n",
    "        else:\n",
    "            days_ago = int(source_time.replace('天前', ''))\n",
    "            source_date = crawl_date - timedelta(days=days_ago)\n",
    "\n",
    "        # 將計算得到的source_date填入「source_date」欄位\n",
    "        df_tw.at[index, 'source_date'] = source_date.strftime('%Y/%m/%d')\n",
    "\n",
    "    # 儲存處理後的資料至 CSV 檔案\n",
    "    df_tw.to_csv(csvfile, index=False, encoding='utf-8-sig')\n",
    "    # 回傳處理後的資料框\n",
    "    return df_tw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_symbols_tw(csvfile):\n",
    "    clear_output(wait=True)\n",
    "    global csv_file_tw\n",
    "    df_tw = pd.read_csv(csvfile)\n",
    "\n",
    "    # 逐行處理資料\n",
    "    for index, row in df_tw.iterrows():\n",
    "        title = row['title']\n",
    "        if '【公告】' in title:\n",
    "            df_tw = df_tw.drop(index)  # 刪除含有【公告】的列\n",
    "        else:\n",
    "            clean_title = re.sub(r'【.*?】|《.*?》', '', title)\n",
    "            df_tw.at[index, 'title'] = clean_title\n",
    "\n",
    "    # 檢查重複的title並保留最後一筆\n",
    "    df_tw = df_tw.drop_duplicates(subset='title', keep='last')\n",
    "    #df.drop('source_time', axis=1, inplace=True)\n",
    "    df_tw = df_tw[['crawl_date', 'source_date', 'title', 'source', 'source_time', 'Sentiment_label']]\n",
    "\n",
    "    # 按舊至新重新排序\n",
    "    df_tw = df_tw.sort_values('source_date')\n",
    "\n",
    "    # 儲存處理後的資料至 CSV 檔案\n",
    "    df_tw.to_csv(csvfile, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    # 回傳處理後的資料框\n",
    "    return df_tw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "爬蟲資料已儲存至news_tw.csv檔案中。\n"
     ]
    }
   ],
   "source": [
    "crawler_tw()\n",
    "df_tw = source_date_tw(csv_file_tw)\n",
    "df_tw = remove_symbols_tw(csv_file_tw)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 美國新聞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawler_us(csv_file_us=\"news_us.csv\"):\n",
    "\n",
    "    # Yahoo Finance RSS 來源(一般爬蟲方法被阻擋)\n",
    "    feeds = [\n",
    "        \"https://finance.yahoo.com/rss/topstories\",\n",
    "        \"https://finance.yahoo.com/news/rss\"\n",
    "    ]\n",
    "\n",
    "    all_entries = []\n",
    "\n",
    "    for feed_url in feeds:\n",
    "        feed = feedparser.parse(feed_url)\n",
    "        all_entries.extend(feed.entries)\n",
    "\n",
    "    if not all_entries:\n",
    "        print(\"⚠️ 沒有抓到任何新聞。\")\n",
    "        return\n",
    "\n",
    "    # 構建資料框架\n",
    "    crawl_time = datetime.now().strftime(\"%Y/%m/%d %H:%M\")\n",
    "    df = pd.DataFrame([{\n",
    "        \"crawl_date\": crawl_time,\n",
    "        \"source_time\": e.published if 'published' in e else None,\n",
    "        \"title\": e.title,\n",
    "        \"source\": \"Yahoo Finance RSS\",\n",
    "        \"Sentiment_label\": \"\",\n",
    "        \"Positive_sentim\": \"\",\n",
    "        \"Negative_sentim\": \"\",\n",
    "        \"Neutral_sentim\": \"\",\n",
    "        \"link\": e.link\n",
    "    } for e in all_entries])\n",
    "\n",
    "    # 若 CSV 存在，讀取原資料並合併\n",
    "    if os.path.exists(csv_file_us):\n",
    "        df_old = pd.read_csv(csv_file_us, encoding=\"utf-8-sig\")\n",
    "        df = pd.concat([df_old, df], ignore_index=True)\n",
    "\n",
    "    # 移除重複標題\n",
    "    df.drop_duplicates(subset=[\"title\"], inplace=True)\n",
    "\n",
    "    # 輸出 CSV\n",
    "    df.to_csv(csv_file_us, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"✅ 爬蟲資料已儲存至 {csv_file_us}，共 {len(df)} 筆新聞。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def source_date_us(csvfile):\n",
    "    \n",
    "    df_us = pd.read_csv(csvfile)\n",
    "\n",
    "    for index, row in df_us.iterrows():\n",
    "        crawl_date = datetime.strptime(row['crawl_date'], '%Y/%m/%d %H:%M')\n",
    "        source_time = str(row['source_time']).strip()\n",
    "\n",
    "        try:\n",
    "            # RSS ISO 日期格式\n",
    "            source_date = datetime.strptime(source_time, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "        except (ValueError, TypeError):\n",
    "            # 原本「幾秒前/分鐘/小時/天前」處理\n",
    "            if 'second ago' in source_time or 'seconds ago' in source_time:\n",
    "                sec = int(re.search(r'\\d+', source_time).group())\n",
    "                source_date = crawl_date - timedelta(seconds=sec)\n",
    "            elif 'minute ago' in source_time or 'minutes ago' in source_time:\n",
    "                min = int(re.search(r'\\d+', source_time).group())\n",
    "                source_date = crawl_date - timedelta(minutes=min)\n",
    "            elif 'hour ago' in source_time or 'hours ago' in source_time:\n",
    "                hr = int(re.search(r'\\d+', source_time).group())\n",
    "                source_date = crawl_date - timedelta(hours=hr)\n",
    "            elif source_time == 'yesterday':\n",
    "                source_date = crawl_date - timedelta(days=1)\n",
    "            else:\n",
    "                # fallback: 天數\n",
    "                m = re.search(r'\\d+', source_time)\n",
    "                days_ago = int(m.group()) if m else 0\n",
    "                source_date = crawl_date - timedelta(days=days_ago)\n",
    "\n",
    "        # 填入 source_date\n",
    "        df_us.at[index, 'source_date'] = source_date.strftime('%Y/%m/%d')\n",
    "\n",
    "    # 清理標題重複\n",
    "    df_us['title'] = df_us['title'].str.replace(\"‘|’\", \"'\", regex=True)\n",
    "    df_us['title'] = df_us['title'].str.replace(\"—\", \"-\", regex=True)\n",
    "    df_us = df_us.drop_duplicates(subset='title', keep='last')\n",
    "\n",
    "    # 保留欄位順序\n",
    "    df_us = df_us[['crawl_date', 'source_date', 'title', 'source', 'source_time', \n",
    "                   'Sentiment_label', 'Positive_sentim', 'Negative_sentim', 'Neutral_sentim']]\n",
    "\n",
    "    # 按日期排序\n",
    "    df_us = df_us.sort_values('source_date')\n",
    "\n",
    "    # 儲存 CSV\n",
    "    df_us.to_csv(csvfile, index=False, encoding='utf-8-sig')\n",
    "    return df_us\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 排程器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 爬蟲資料已儲存至 news_us.csv，共 49 筆新聞。\n"
     ]
    }
   ],
   "source": [
    "crawler_us()\n",
    "df_us = source_date_us(csv_file_us)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def job():\n",
    "#     clear_output(wait=True)\n",
    "#     crawler_tw()\n",
    "#     df_tw = source_date_tw(csv_file_tw)\n",
    "#     df_tw = remove_symbols_tw(csv_file_tw)\n",
    "#     crawler_us()\n",
    "#     df_us = source_date_us(csv_file_us)\n",
    "#     print(f\"新聞爬蟲完成時間：{datetime.now().strftime('%Y/%m/%d %H:%M')}\")\n",
    "\n",
    "# schedule.every().day.at(\"14:00\").do(job)\n",
    "\n",
    "# while True:\n",
    "#     schedule.run_pending()\n",
    "#     time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
